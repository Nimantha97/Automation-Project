{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31d08141-60b6-4386-b7ee-0eb74a0d589c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#getting source file and creating creating week vise excel files with rfm score get cluster and segmentation\n",
    "#-----------------------------------1st script-----------------------------------------------------------------------------------\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import re\n",
    "\n",
    "# Set the paths for the input and output folders\n",
    "input_folder = \"C:/Users/GCV/Documents/MAS/Project AR/Testing/input_test/test/\"\n",
    "output_folder = \"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/weekly_input_data/\"\n",
    "\n",
    "# Set the path of the folder you want to get file names from\n",
    "folder_path = \"C:/Users/GCV/Documents/MAS/Project AR/Testing/input_test/test/\"\n",
    "\n",
    "# Set the file pattern you want to match, for example, all files with \".txt\" extension\n",
    "\n",
    "file_pattern_xlsx = folder_path + \"*.xlsx\"\n",
    "file_pattern_xls = folder_path + \"*.xls\"\n",
    "file_pattern_xlsm = folder_path + \"*.xlsm\"\n",
    "\n",
    "# Use glob function to get the file names that match each file pattern\n",
    "file_names_xlsx = glob.glob(file_pattern_xlsx)\n",
    "file_names_xls = glob.glob(file_pattern_xls)\n",
    "file_names_xlsm = glob.glob(file_pattern_xlsm)\n",
    "\n",
    "\n",
    "# Loop through each file name and process the corresponding Excel file\n",
    "for file_name in file_names_xlsx:\n",
    "        for file in glob.glob('./*.xlsx'):\n",
    "                if '~$' in file:\n",
    "                    continue\n",
    "\n",
    "        \n",
    "        # Read the Excel file into a pandas DataFrame\n",
    "        df = pd.read_excel(file_name,sheet_name='Breakdown External ')\n",
    "\n",
    "        #creating cp pivot table\n",
    "        pivot_table_cp = pd.pivot_table(df, values='CP',index=['Customer Name'] , aggfunc = 'max')\n",
    "       \n",
    "    \n",
    "        # Get a boolean mask of the columns that contain \"Amount\"\n",
    "        # mask = df.columns.str.contains(\"Amount\")\n",
    "        # df.loc[:, mask] = df.loc[:, mask].rename(columns=lambda x: x.replace(\"Amount\", \"Amount in USD\"))\n",
    "\n",
    "        \n",
    "        #rename some changeable column names into a unique name\n",
    "        if 'Amount' in df.columns:\n",
    "            df = df.rename(columns={'Amount': 'Amount in USD'})\n",
    "        elif 'Amount in local currency' in df.columns:\n",
    "            df = df.rename(columns={'Amount in local currency': 'Amount in USD'})\n",
    "        elif \"Ageing as at 27th October'22\" in df.columns:\n",
    "            df = df.rename(columns={\"Ageing as at 27th October'22\": 'Ageing'})\n",
    "        \n",
    "            \n",
    "        #convert 'Amount in USD' column's data type into numeric\n",
    "        df['Amount in USD'] = pd.to_numeric(df['Amount in USD'], errors='coerce')\n",
    "        #creating ageing pivot table\n",
    "        #print(file_name)\n",
    "        pivot_table_ageing = pd.pivot_table(df, values='Amount in USD',index=['Customer Name'] , columns=['Ageing'], aggfunc = 'sum')\n",
    "\n",
    "        #filling null values with zeros\n",
    "        pivot_table_ageing = pivot_table_ageing.fillna(0)\n",
    "\n",
    "        #concat 2 pivot tables \n",
    "        summary_df=pd.concat([pivot_table_ageing, pivot_table_cp['CP']], axis=1)\n",
    "\n",
    "        #removing negative values in data frame\n",
    "        summary_df = summary_df.applymap(lambda x: max(0, x))\n",
    "\n",
    "        summary_df['Customer Name'] = summary_df.index\n",
    "        summary_df = summary_df.reset_index(drop=True)\n",
    "       \n",
    "        #some column name has 0-30 Days so remove Days from name\n",
    "        summary_df.columns = summary_df.columns.str.replace(' Days', '')\n",
    "\n",
    "        if 'Above 120' in summary_df.columns:\n",
    "            summary_df = summary_df.rename(columns={'Above 120': '120 Above'})\n",
    "\n",
    "        summary_subset1 = summary_df[['CP', '0-30', '31-45', '46-60','61-90','91-120','120 Above']]\n",
    "        summary_subset2 = summary_df[['Customer Name']]\n",
    "        def replace_values(row):\n",
    "            if row['CP'] > 30:\n",
    "                row['0-30'] = 0\n",
    "            if row['CP'] > 45:\n",
    "                row['31-45'] = 0\n",
    "            if row['CP'] > 60:\n",
    "                row['46-60'] = 0\n",
    "            if row['CP'] > 90:\n",
    "                row['61-90'] = 0\n",
    "            if row['CP'] > 120:\n",
    "                row['91-120'] = 0\n",
    "            if row['CP'] > 120:\n",
    "                row['120 Above'] = 0\n",
    "\n",
    "            return row\n",
    "\n",
    "        # apply the function to each row of the data frame\n",
    "        summary_subset1 = summary_subset1.apply(replace_values, axis=1)\n",
    "        summary_df = pd.concat([summary_subset2,summary_subset1],axis=1)\n",
    "        \n",
    "        # Convert all column names to strings\n",
    "        df.columns = df.columns.map(str)\n",
    "\n",
    "        # Define the regular expression to match the date format\n",
    "        date_regex = r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}'\n",
    "\n",
    "        # Find the column name that matches the date format\n",
    "        date_col_name = next((col for col in df.columns if re.match(date_regex, col)), None)\n",
    "\n",
    "        # Rename the date column to a more appropriate name\n",
    "        if date_col_name:\n",
    "            df.rename(columns={date_col_name: 'overdue_dates'}, inplace=True)\n",
    "\n",
    "        #df.columns = original_column_labels\n",
    "        #df.columns = df.columns.tolist()\n",
    "        #print(df.columns)\n",
    "\n",
    "        df_monetary = df[['Customer Name','Amount in USD','CP','overdue_dates']]\n",
    "        #df_monetary.head()\n",
    "\n",
    "\n",
    "        # define a function to apply to 'overdue_dates' column and create the new column\n",
    "        def create_new_col(row):\n",
    "            if row['overdue_dates'] > row['CP']:\n",
    "                return row['Amount in USD']\n",
    "            else:\n",
    "                return 0\n",
    "   \n",
    "        # apply the function to create the new column\n",
    "        df_monetary['M'] = df_monetary.apply(create_new_col, axis=1)\n",
    "\n",
    "        # # Convert negative values in column  to 0\n",
    "        # df_monetary['M'] = df_monetary['M'].clip(lower=0)\n",
    "        # df_monetary['M'] = df_monetary['M'].clip(lower=0)\n",
    "        \n",
    "        \n",
    "        #print(df_monetary.head())\n",
    "\n",
    "\n",
    "        pivot_table_monetary = pd.pivot_table(df_monetary, values=['Amount in USD', 'M'],index=['Customer Name'] , aggfunc = 'sum')\n",
    "        pivot_table_monetary = pivot_table_monetary.rename(columns={'Amount in USD': 'Total Receivable'})\n",
    "\n",
    "        \n",
    "        #print(pivot_table_monetary.head())\n",
    "        \n",
    "        \n",
    "        \n",
    "        #calculating frequency\n",
    "        # define a custom function to count the number of values greater than 0 in a list\n",
    "        def freq(lst):\n",
    "            return sum([1 for x in lst if x > 0])\n",
    "        # create new subsets of the DataFrame using .loc\n",
    "        df_subset4 = summary_df.loc[:, ['0-30', '31-45', '46-60', '61-90', '91-120', '120 Above']]\n",
    "        df_subset6 = summary_df.loc[:, ['0-30', '31-45', '46-60', '61-90', '91-120', '120 Above']]\n",
    "        df_subset5 = summary_df.loc[:, ['Customer Name', 'CP']]\n",
    "\n",
    "        # create new columns using .loc and avoid chained indexing\n",
    "        df_subset4.loc[:, 'F'] = df_subset4.apply(lambda row: freq(row.tolist()), axis=1)\n",
    "        df_subset6.loc[:, 'RowSum'] = df_subset6.sum(axis=1)\n",
    "        #df_subset4.loc[:, 'M'] = df_subset6.loc[:, 'RowSum']\n",
    "\n",
    "        # concatenate the subsets into a new DataFrame\n",
    "        summary_df = pd.concat([df_subset5, df_subset4], axis=1)\n",
    "        summary_df = pd.merge(summary_df,pivot_table_monetary,on='Customer Name')\n",
    "        #print(summary_df.head())\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        df_subset7 = summary_df.loc[:, ['0-30', '31-45', '46-60', '61-90', '91-120', '120 Above','M','Total Receivable']]\n",
    "        df_subset8 = summary_df.loc[:, ['Customer Name', 'CP','F']]\n",
    "\n",
    "        def calc_R(row):\n",
    "            # Calculate the divisor (total value for the row)\n",
    "            total = row['M']\n",
    "            # Calculate the weights and apply to the values in the row\n",
    "            if total == 0:\n",
    "                return None\n",
    "            else:\n",
    "                weights = [0.083333333, 2.5, 3.75, 5, 7.5, 10]\n",
    "                values = [row['0-30'], row['31-45'], row['46-60'], row['61-90'], row['91-120'], row['120 Above']]\n",
    "                weighted_values = [value * weight / total for value, weight in zip(values, weights)]\n",
    "                # Sum the weighted values to get the final R value\n",
    "                return sum(weighted_values)\n",
    "\n",
    "        # Apply the custom function to each row and store the result in a new column 'R'\n",
    "        df_subset7['R'] = df_subset7.apply(calc_R, axis=1)\n",
    "        df_subset7.head()\n",
    "        summary_df = pd.concat([df_subset8, df_subset7], axis=1)\n",
    "        summary_df=summary_df.fillna(0)\n",
    "        summary_df = summary_df[['Customer Name', 'CP' , '0-30', '31-45', '46-60', '61-90',\n",
    "               '91-120', '120 Above', 'R', 'F','M','Total Receivable']]\n",
    "        nature_df = pd.read_excel(r'C:\\Users\\GCV\\Documents\\MAS\\Project AR\\Automation\\Nature.xlsx')\n",
    "            #left join on both data frame\n",
    "        merged_df = pd.merge(summary_df ,nature_df ,on=\"Customer Name\", how=\"left\") #both column name of customer shoul be same if not remane both column as customer\n",
    "        merged_df[\"nature of the business\"].fillna(0, inplace=True)\n",
    "        summary_df['N'] = merged_df[\"nature of the business\"]\n",
    "        #drop rows where N and M column vlaues 0\n",
    "        mask1 = summary_df['N'] != 0 \n",
    "        summary_df = summary_df[mask1]\n",
    "        mask2 = summary_df['M'] != 0\n",
    "        summary_df = summary_df[mask2]\n",
    "\n",
    "        summary_df =summary_df.reset_index(drop=True)\n",
    "\n",
    "        final_df = summary_df[['Customer Name','R','F','M','N','Total Receivable']]\n",
    "        #print(final_df.head())\n",
    "        #final_df.to_excel('W16.xlsx',index=False)\n",
    "        \n",
    "        #convert negative values into 0\n",
    "        def convert_negatives_to_zero(df, col_name):\n",
    "            df[col_name] = df[col_name].apply(lambda x: max(0, x))\n",
    "            return df\n",
    "        \n",
    "        final_df = convert_negatives_to_zero(final_df,'M')\n",
    "        final_df = convert_negatives_to_zero(final_df,'Total Receivable')\n",
    "        \n",
    "        \n",
    "        #print(file_name)\n",
    "        # Construct the output file name based on the input file name\n",
    "        output_file_name = os.path.join(output_folder, os.path.basename(file_name))\n",
    "        output_file_name = os.path.splitext(output_file_name)[0] + \".xlsx\"\n",
    "        # Write the processed DataFrame to the output file\n",
    "        final_df.to_excel(output_file_name, index=False)# Convert all column names to strings\n",
    "        \n",
    "#------------------------------xls files-----------------------------------------------------------------------------        \n",
    "# Loop through each file name and process the corresponding Excel file\n",
    "for file_name in file_names_xls:\n",
    "        for file in glob.glob('./*.xls'):\n",
    "                if '~$' in file:\n",
    "                    continue\n",
    "\n",
    "        \n",
    "        # Read the Excel file into a pandas DataFrame\n",
    "        df = pd.read_excel(file_name,sheet_name='Breakdown External ')\n",
    "\n",
    "        #creating cp pivot table\n",
    "        pivot_table_cp = pd.pivot_table(df, values='CP',index=['Customer Name'] , aggfunc = 'max')\n",
    "       \n",
    "    \n",
    "        # Get a boolean mask of the columns that contain \"Amount\"\n",
    "        # mask = df.columns.str.contains(\"Amount\")\n",
    "        # df.loc[:, mask] = df.loc[:, mask].rename(columns=lambda x: x.replace(\"Amount\", \"Amount in USD\"))\n",
    "\n",
    "        \n",
    "        #rename some changeable column names into a unique name\n",
    "        if 'Amount' in df.columns:\n",
    "            df = df.rename(columns={'Amount': 'Amount in USD'})\n",
    "        elif 'Amount in local currency' in df.columns:\n",
    "            df = df.rename(columns={'Amount in local currency': 'Amount in USD'})\n",
    "        elif \"Ageing as at 27th October'22\" in df.columns:\n",
    "            df = df.rename(columns={\"Ageing as at 27th October'22\": 'Ageing'})\n",
    "        \n",
    "            \n",
    "        #convert 'Amount in USD' column's data type into numeric\n",
    "        df['Amount in USD'] = pd.to_numeric(df['Amount in USD'], errors='coerce')\n",
    "        #creating ageing pivot table\n",
    "        #print(file_name)\n",
    "        pivot_table_ageing = pd.pivot_table(df, values='Amount in USD',index=['Customer Name'] , columns=['Ageing'], aggfunc = 'sum')\n",
    "\n",
    "        #filling null values with zeros\n",
    "        pivot_table_ageing = pivot_table_ageing.fillna(0)\n",
    "\n",
    "        #concat 2 pivot tables \n",
    "        summary_df=pd.concat([pivot_table_ageing, pivot_table_cp['CP']], axis=1)\n",
    "\n",
    "        #removing negative values in data frame\n",
    "        summary_df = summary_df.applymap(lambda x: max(0, x))\n",
    "\n",
    "        summary_df['Customer Name'] = summary_df.index\n",
    "        summary_df = summary_df.reset_index(drop=True)\n",
    "       \n",
    "        #some column name has 0-30 Days so remove Days from name\n",
    "        summary_df.columns = summary_df.columns.str.replace(' Days', '')\n",
    "\n",
    "        if 'Above 120' in summary_df.columns:\n",
    "            summary_df = summary_df.rename(columns={'Above 120': '120 Above'})\n",
    "\n",
    "        summary_subset1 = summary_df[['CP', '0-30', '31-45', '46-60','61-90','91-120','120 Above']]\n",
    "        summary_subset2 = summary_df[['Customer Name']]\n",
    "        def replace_values(row):\n",
    "            if row['CP'] > 30:\n",
    "                row['0-30'] = 0\n",
    "            if row['CP'] > 45:\n",
    "                row['31-45'] = 0\n",
    "            if row['CP'] > 60:\n",
    "                row['46-60'] = 0\n",
    "            if row['CP'] > 90:\n",
    "                row['61-90'] = 0\n",
    "            if row['CP'] > 120:\n",
    "                row['91-120'] = 0\n",
    "            if row['CP'] > 120:\n",
    "                row['120 Above'] = 0\n",
    "\n",
    "            return row\n",
    "\n",
    "        # apply the function to each row of the data frame\n",
    "        summary_subset1 = summary_subset1.apply(replace_values, axis=1)\n",
    "        summary_df = pd.concat([summary_subset2,summary_subset1],axis=1)\n",
    "        \n",
    "        # Convert all column names to strings\n",
    "        df.columns = df.columns.map(str)\n",
    "\n",
    "        # Define the regular expression to match the date format\n",
    "        date_regex = r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}'\n",
    "\n",
    "        # Find the column name that matches the date format\n",
    "        date_col_name = next((col for col in df.columns if re.match(date_regex, col)), None)\n",
    "\n",
    "        # Rename the date column to a more appropriate name\n",
    "        if date_col_name:\n",
    "            df.rename(columns={date_col_name: 'overdue_dates'}, inplace=True)\n",
    "\n",
    "        #df.columns = original_column_labels\n",
    "        #df.columns = df.columns.tolist()\n",
    "        #print(df.columns)\n",
    "\n",
    "        df_monetary = df[['Customer Name','Amount in USD','CP','overdue_dates']]\n",
    "        #df_monetary.head()\n",
    "\n",
    "\n",
    "        # define a function to apply to 'overdue_dates' column and create the new column\n",
    "        def create_new_col(row):\n",
    "            if row['overdue_dates'] > row['CP']:\n",
    "                return row['Amount in USD']\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        # apply the function to create the new column\n",
    "        df_monetary['M'] = df_monetary.apply(create_new_col, axis=1)\n",
    "\n",
    "        #print(df_monetary.head())\n",
    "\n",
    "\n",
    "        pivot_table_monetary = pd.pivot_table(df_monetary, values=['Amount in USD', 'M'],index=['Customer Name'] , aggfunc = 'sum')\n",
    "        pivot_table_monetary = pivot_table_monetary.rename(columns={'Amount in USD': 'Total Receivable'})\n",
    "\n",
    "        \n",
    "        #print(pivot_table_monetary.head())\n",
    "        \n",
    "        \n",
    "        \n",
    "        #calculating frequency\n",
    "        # define a custom function to count the number of values greater than 0 in a list\n",
    "        def freq(lst):\n",
    "            return sum([1 for x in lst if x > 0])\n",
    "        # create new subsets of the DataFrame using .loc\n",
    "        df_subset4 = summary_df.loc[:, ['0-30', '31-45', '46-60', '61-90', '91-120', '120 Above']]\n",
    "        df_subset6 = summary_df.loc[:, ['0-30', '31-45', '46-60', '61-90', '91-120', '120 Above']]\n",
    "        df_subset5 = summary_df.loc[:, ['Customer Name', 'CP']]\n",
    "\n",
    "        # create new columns using .loc and avoid chained indexing\n",
    "        df_subset4.loc[:, 'F'] = df_subset4.apply(lambda row: freq(row.tolist()), axis=1)\n",
    "        df_subset6.loc[:, 'RowSum'] = df_subset6.sum(axis=1)\n",
    "        #df_subset4.loc[:, 'M'] = df_subset6.loc[:, 'RowSum']\n",
    "\n",
    "        # concatenate the subsets into a new DataFrame\n",
    "        summary_df = pd.concat([df_subset5, df_subset4], axis=1)\n",
    "        summary_df = pd.merge(summary_df,pivot_table_monetary,on='Customer Name')\n",
    "        #print(summary_df.head())\n",
    "       \n",
    "        # Convert negative values in column  to 0\n",
    "        summary_df['M'] = summary_df['M'].clip(lower=0)\n",
    "        summary_df['Total Receivable'] = summary_df['Total Receivable'].clip(lower=0)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        df_subset7 = summary_df.loc[:, ['0-30', '31-45', '46-60', '61-90', '91-120', '120 Above','M','Total Receivable']]\n",
    "        df_subset8 = summary_df.loc[:, ['Customer Name', 'CP','F']]\n",
    "\n",
    "        def calc_R(row):\n",
    "            # Calculate the divisor (total value for the row)\n",
    "            total = row['M']\n",
    "            # Calculate the weights and apply to the values in the row\n",
    "            if total == 0:\n",
    "                return None\n",
    "            else:\n",
    "                weights = [0.083333333, 2.5, 3.75, 5, 7.5, 10]\n",
    "                values = [row['0-30'], row['31-45'], row['46-60'], row['61-90'], row['91-120'], row['120 Above']]\n",
    "                weighted_values = [value * weight / total for value, weight in zip(values, weights)]\n",
    "                # Sum the weighted values to get the final R value\n",
    "                return sum(weighted_values)\n",
    "\n",
    "        # Apply the custom function to each row and store the result in a new column 'R'\n",
    "        df_subset7['R'] = df_subset7.apply(calc_R, axis=1)\n",
    "        df_subset7.head()\n",
    "        summary_df = pd.concat([df_subset8, df_subset7], axis=1)\n",
    "        summary_df=summary_df.fillna(0)\n",
    "        summary_df = summary_df[['Customer Name', 'CP' , '0-30', '31-45', '46-60', '61-90',\n",
    "               '91-120', '120 Above', 'R', 'F','M','Total Receivable']]\n",
    "        nature_df = pd.read_excel(r'C:\\Users\\GCV\\Documents\\MAS\\Project AR\\Automation\\Nature.xlsx')\n",
    "            #left join on both data frame\n",
    "        merged_df = pd.merge(summary_df ,nature_df ,on=\"Customer Name\", how=\"left\") #both column name of customer shoul be same if not remane both column as customer\n",
    "        merged_df[\"nature of the business\"].fillna(0, inplace=True)\n",
    "        summary_df['N'] = merged_df[\"nature of the business\"]\n",
    "        #drop rows where N and M column vlaues 0\n",
    "        mask1 = summary_df['N'] != 0 \n",
    "        summary_df = summary_df[mask1]\n",
    "        mask2 = summary_df['M'] != 0\n",
    "        summary_df = summary_df[mask2]\n",
    "\n",
    "        summary_df =summary_df.reset_index(drop=True)\n",
    "\n",
    "        final_df = summary_df[['Customer Name','R','F','M','N','Total Receivable']]\n",
    "        #print(final_df.head())\n",
    "        #final_df.to_excel('W16.xlsx',index=False)\n",
    "\n",
    "        \n",
    "        #convert negative values into 0\n",
    "        def convert_negatives_to_zero(df, col_name):\n",
    "            df[col_name] = df[col_name].apply(lambda x: max(0, x))\n",
    "            return df\n",
    "        \n",
    "        final_df = convert_negatives_to_zero(final_df,'M')\n",
    "        final_df = convert_negatives_to_zero(final_df,'Total Receivable')\n",
    "\n",
    "        \n",
    "        \n",
    "        #print(file_name)\n",
    "        # Construct the output file name based on the input file name\n",
    "        output_file_name = os.path.join(output_folder, os.path.basename(file_name))\n",
    "        output_file_name = os.path.splitext(output_file_name)[0] + \".xlsx\"\n",
    "        # Write the processed DataFrame to the output file\n",
    "        final_df.to_excel(output_file_name, index=False)# Convert all column names to strings\n",
    " \n",
    "        \n",
    "#------------------------xlsm-----------------------------------------------------------------------------------       \n",
    " # Loop through each file name and process the corresponding Excel file\n",
    "for file_name in file_names_xlsm:\n",
    "        for file in glob.glob('./*.xlsm'):\n",
    "                if '~$' in file:\n",
    "                    continue\n",
    "\n",
    "        \n",
    "        # Read the Excel file into a pandas DataFrame\n",
    "        df = pd.read_excel(file_name,sheet_name='Breakdown External ')\n",
    "\n",
    "        #creating cp pivot table\n",
    "        pivot_table_cp = pd.pivot_table(df, values='CP',index=['Customer Name'] , aggfunc = 'max')\n",
    "       \n",
    "    \n",
    "        # Get a boolean mask of the columns that contain \"Amount\"\n",
    "        # mask = df.columns.str.contains(\"Amount\")\n",
    "        # df.loc[:, mask] = df.loc[:, mask].rename(columns=lambda x: x.replace(\"Amount\", \"Amount in USD\"))\n",
    "\n",
    "        \n",
    "        #rename some changeable column names into a unique name\n",
    "        if 'Amount' in df.columns:\n",
    "            df = df.rename(columns={'Amount': 'Amount in USD'})\n",
    "        elif 'Amount in local currency' in df.columns:\n",
    "            df = df.rename(columns={'Amount in local currency': 'Amount in USD'})\n",
    "        elif \"Ageing as at 27th October'22\" in df.columns:\n",
    "            df = df.rename(columns={\"Ageing as at 27th October'22\": 'Ageing'})\n",
    "        \n",
    "            \n",
    "        #convert 'Amount in USD' column's data type into numeric\n",
    "        df['Amount in USD'] = pd.to_numeric(df['Amount in USD'], errors='coerce')\n",
    "        #creating ageing pivot table\n",
    "        #print(file_name)\n",
    "        pivot_table_ageing = pd.pivot_table(df, values='Amount in USD',index=['Customer Name'] , columns=['Ageing'], aggfunc = 'sum')\n",
    "\n",
    "        #filling null values with zeros\n",
    "        pivot_table_ageing = pivot_table_ageing.fillna(0)\n",
    "\n",
    "        #concat 2 pivot tables \n",
    "        summary_df=pd.concat([pivot_table_ageing, pivot_table_cp['CP']], axis=1)\n",
    "\n",
    "        #removing negative values in data frame\n",
    "        summary_df = summary_df.applymap(lambda x: max(0, x))\n",
    "\n",
    "        summary_df['Customer Name'] = summary_df.index\n",
    "        summary_df = summary_df.reset_index(drop=True)\n",
    "       \n",
    "        #some column name has 0-30 Days so remove Days from name\n",
    "        summary_df.columns = summary_df.columns.str.replace(' Days', '')\n",
    "\n",
    "        if 'Above 120' in summary_df.columns:\n",
    "            summary_df = summary_df.rename(columns={'Above 120': '120 Above'})\n",
    "\n",
    "        summary_subset1 = summary_df[['CP', '0-30', '31-45', '46-60','61-90','91-120','120 Above']]\n",
    "        summary_subset2 = summary_df[['Customer Name']]\n",
    "        def replace_values(row):\n",
    "            if row['CP'] > 30:\n",
    "                row['0-30'] = 0\n",
    "            if row['CP'] > 45:\n",
    "                row['31-45'] = 0\n",
    "            if row['CP'] > 60:\n",
    "                row['46-60'] = 0\n",
    "            if row['CP'] > 90:\n",
    "                row['61-90'] = 0\n",
    "            if row['CP'] > 120:\n",
    "                row['91-120'] = 0\n",
    "            if row['CP'] > 120:\n",
    "                row['120 Above'] = 0\n",
    "\n",
    "            return row\n",
    "\n",
    "        # apply the function to each row of the data frame\n",
    "        summary_subset1 = summary_subset1.apply(replace_values, axis=1)\n",
    "        summary_df = pd.concat([summary_subset2,summary_subset1],axis=1)\n",
    "        \n",
    "        # Convert all column names to strings\n",
    "        df.columns = df.columns.map(str)\n",
    "\n",
    "        # Define the regular expression to match the date format\n",
    "        date_regex = r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}'\n",
    "\n",
    "        # Find the column name that matches the date format\n",
    "        date_col_name = next((col for col in df.columns if re.match(date_regex, col)), None)\n",
    "\n",
    "        # Rename the date column to a more appropriate name\n",
    "        if date_col_name:\n",
    "            df.rename(columns={date_col_name: 'overdue_dates'}, inplace=True)\n",
    "\n",
    "        #df.columns = original_column_labels\n",
    "        #df.columns = df.columns.tolist()\n",
    "        #print(df.columns)\n",
    "\n",
    "        df_monetary = df[['Customer Name','Amount in USD','CP','overdue_dates']]\n",
    "        #df_monetary.head()\n",
    "\n",
    "\n",
    "        # define a function to apply to 'overdue_dates' column and create the new column\n",
    "        def create_new_col(row):\n",
    "            if row['overdue_dates'] > row['CP']:\n",
    "                return row['Amount in USD']\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        # apply the function to create the new column\n",
    "        df_monetary['M'] = df_monetary.apply(create_new_col, axis=1)\n",
    "\n",
    "        #print(df_monetary.head())\n",
    "\n",
    "\n",
    "        pivot_table_monetary = pd.pivot_table(df_monetary, values=['Amount in USD', 'M'],index=['Customer Name'] , aggfunc = 'sum')\n",
    "        pivot_table_monetary = pivot_table_monetary.rename(columns={'Amount in USD': 'Total Receivable'})\n",
    "\n",
    "        \n",
    "        #print(pivot_table_monetary.head())\n",
    "        \n",
    "        \n",
    "        \n",
    "        #calculating frequency\n",
    "        # define a custom function to count the number of values greater than 0 in a list\n",
    "        def freq(lst):\n",
    "            return sum([1 for x in lst if x > 0])\n",
    "        # create new subsets of the DataFrame using .loc\n",
    "        df_subset4 = summary_df.loc[:, ['0-30', '31-45', '46-60', '61-90', '91-120', '120 Above']]\n",
    "        df_subset6 = summary_df.loc[:, ['0-30', '31-45', '46-60', '61-90', '91-120', '120 Above']]\n",
    "        df_subset5 = summary_df.loc[:, ['Customer Name', 'CP']]\n",
    "\n",
    "        # create new columns using .loc and avoid chained indexing\n",
    "        df_subset4.loc[:, 'F'] = df_subset4.apply(lambda row: freq(row.tolist()), axis=1)\n",
    "        df_subset6.loc[:, 'RowSum'] = df_subset6.sum(axis=1)\n",
    "        #df_subset4.loc[:, 'M'] = df_subset6.loc[:, 'RowSum']\n",
    "\n",
    "        # concatenate the subsets into a new DataFrame\n",
    "        summary_df = pd.concat([df_subset5, df_subset4], axis=1)\n",
    "        summary_df = pd.merge(summary_df,pivot_table_monetary,on='Customer Name')\n",
    "        #print(summary_df.head())\n",
    "       \n",
    "        # Convert negative values in column  to 0\n",
    "        summary_df['M'] = summary_df['M'].clip(lower=0)\n",
    "        summary_df['Total Receivable'] = summary_df['Total Receivable'].clip(lower=0)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        df_subset7 = summary_df.loc[:, ['0-30', '31-45', '46-60', '61-90', '91-120', '120 Above','M','Total Receivable']]\n",
    "        df_subset8 = summary_df.loc[:, ['Customer Name', 'CP','F']]\n",
    "\n",
    "        def calc_R(row):\n",
    "            # Calculate the divisor (total value for the row)\n",
    "            total = row['M']\n",
    "            # Calculate the weights and apply to the values in the row\n",
    "            if total == 0:\n",
    "                return None\n",
    "            else:\n",
    "                weights = [0.083333333, 2.5, 3.75, 5, 7.5, 10]\n",
    "                values = [row['0-30'], row['31-45'], row['46-60'], row['61-90'], row['91-120'], row['120 Above']]\n",
    "                weighted_values = [value * weight / total for value, weight in zip(values, weights)]\n",
    "                # Sum the weighted values to get the final R value\n",
    "                return sum(weighted_values)\n",
    "\n",
    "        # Apply the custom function to each row and store the result in a new column 'R'\n",
    "        df_subset7['R'] = df_subset7.apply(calc_R, axis=1)\n",
    "        df_subset7.head()\n",
    "        summary_df = pd.concat([df_subset8, df_subset7], axis=1)\n",
    "        summary_df=summary_df.fillna(0)\n",
    "        summary_df = summary_df[['Customer Name', 'CP' , '0-30', '31-45', '46-60', '61-90',\n",
    "               '91-120', '120 Above', 'R', 'F','M','Total Receivable']]\n",
    "        nature_df = pd.read_excel(r'C:\\Users\\GCV\\Documents\\MAS\\Project AR\\Automation\\Nature.xlsx')\n",
    "            #left join on both data frame\n",
    "        merged_df = pd.merge(summary_df ,nature_df ,on=\"Customer Name\", how=\"left\") #both column name of customer shoul be same if not remane both column as customer\n",
    "        merged_df[\"nature of the business\"].fillna(0, inplace=True)\n",
    "        summary_df['N'] = merged_df[\"nature of the business\"]\n",
    "        #drop rows where N and M column vlaues 0\n",
    "        mask1 = summary_df['N'] != 0 \n",
    "        summary_df = summary_df[mask1]\n",
    "        mask2 = summary_df['M'] != 0\n",
    "        summary_df = summary_df[mask2]\n",
    "\n",
    "        summary_df =summary_df.reset_index(drop=True)\n",
    "\n",
    "        final_df = summary_df[['Customer Name','R','F','M','N','Total Receivable']]\n",
    "        #print(final_df.head())\n",
    "        #final_df.to_excel('W16.xlsx',index=False)\n",
    "\n",
    "        \n",
    "        #convert negative values into 0\n",
    "        def convert_negatives_to_zero(df, col_name):\n",
    "            df[col_name] = df[col_name].apply(lambda x: max(0, x))\n",
    "            return df\n",
    "        \n",
    "        final_df = convert_negatives_to_zero(final_df,'M')\n",
    "        final_df = convert_negatives_to_zero(final_df,'Total Receivable')\n",
    "\n",
    "        \n",
    "        \n",
    "        #print(file_name)\n",
    "        # Construct the output file name based on the input file name\n",
    "        output_file_name = os.path.join(output_folder, os.path.basename(file_name))\n",
    "        output_file_name = os.path.splitext(output_file_name)[0] + \".xlsx\"\n",
    "        # Write the processed DataFrame to the output file\n",
    "        final_df.to_excel(output_file_name, index=False)# Convert all column names to strings\n",
    "\n",
    "\n",
    "print(\"done\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d8016aa-bb3b-4e27-a13c-b52c2f5bce93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#----------------------------Adding Date column to all week data-(2nd script)-------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Set the paths for the input and output folders\n",
    "input_folder = \"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/weekly_input_data/\"\n",
    "output_folder = \"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/weekly_input_data_with_date/\"\n",
    "\n",
    "\n",
    "# Get a list of all the Excel files in the folder\n",
    "file_list = [f for f in os.listdir(input_folder) if f.endswith('.xlsx')]\n",
    "\n",
    "\n",
    "\n",
    "# Loop through each file name and process the corresponding Excel file\n",
    "pattern = re.compile(r'^(\\d{4})\\sW(\\d+)\\.xlsx$')\n",
    "\n",
    "for file_name in file_list:\n",
    "    \n",
    "        match = pattern.match(file_name)\n",
    "        if match:\n",
    "            year = int(match.group(1))\n",
    "            week_num = int(match.group(2))\n",
    "\n",
    "            # Calculate the date for the Monday of the given week number in the year\n",
    "            date = pd.to_datetime(f'{year}-W{week_num}-1', format='%Y-W%W-%w').date()\n",
    "\n",
    "            # Read the Excel file into a pandas DataFrame\n",
    "            df = pd.read_excel(os.path.join(input_folder, file_name))\n",
    "            #df = df.drop('Date', axis=1)\n",
    "\n",
    "            # Add the date column to the DataFrame\n",
    "            df['date'] = date\n",
    "            #print(df.head(2))\n",
    "\n",
    "            # Save the modified DataFrame back to the Excel file\n",
    "            df.to_excel(os.path.join(output_folder, file_name), index=False)\n",
    "        else:\n",
    "            print(f\"Error: File name '{file_name}' does not match the expected format\")\n",
    "\n",
    "    \n",
    "print(\"done\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "769e29cd-2766-49eb-95f0-dd5ab7ae6da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------calculating RFM score-(3rd script)---------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Set the paths for the input and output folders\n",
    "input_folder = \"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/weekly_input_data_with_date/\"\n",
    "output_folder = \"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/weekly_output_data/\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set the path of the folder you want to get file names from\n",
    "folder_path = \"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/weekly_input_data_with_date/\"\n",
    "\n",
    "# Set the file pattern you want to match, for example, all files with \".txt\" extension\n",
    "\n",
    "file_pattern_xlsx = folder_path + \"*.xlsx\"\n",
    "\n",
    "# Use glob function to get the file names that match each file pattern\n",
    "file_names_xlsx = glob.glob(file_pattern_xlsx)\n",
    "\n",
    "\n",
    "\n",
    "# Loop through each file name and process the corresponding Excel file\n",
    "\n",
    "\n",
    "for file_name in file_names_xlsx:\n",
    "    \n",
    "        rfm_df = pd.read_excel(file_name)\n",
    "        \n",
    "        # print(file_name)\n",
    "        # print(rfm_df)\n",
    "        rfm_df['R_rank'] = rfm_df['R'].rank(ascending=True)\n",
    "        rfm_df['F_rank'] = rfm_df['F'].rank(ascending=True)\n",
    "        rfm_df['M_rank'] = rfm_df['M'].rank(ascending=True)\n",
    "        rfm_df['N_rank'] = rfm_df['N'].rank(ascending=True)\n",
    "        \n",
    "         # normalizing the rank of the customers\n",
    "        rfm_df['R_rank_norm'] = (rfm_df['R_rank']/rfm_df['R_rank'].max())*100\n",
    "        rfm_df['F_rank_norm'] = (rfm_df['F_rank']/rfm_df['F_rank'].max())*100\n",
    "        rfm_df['M_rank_norm'] = (rfm_df['M_rank']/rfm_df['M_rank'].max())*100\n",
    "        rfm_df['N_rank_norm'] = (rfm_df['N_rank']/rfm_df['N_rank'].max())*100\n",
    "\n",
    "        rfm_df.drop(columns=['R_rank', 'F_rank', 'M_rank','N_rank'], inplace=True)\n",
    "\n",
    "        #print('Normalized --------------------------------------------------------------------------------: ',rfm_df)\n",
    "        \n",
    "        rfm_df['RFM_Score'] = 0.25*rfm_df['R_rank_norm']+0.25 *rfm_df['F_rank_norm']+0.25*rfm_df['M_rank_norm']+0.25*rfm_df['R_rank_norm']\n",
    "        rfm_df['RFM_Score'] *= 0.05\n",
    "        #rfm_df = rfm_df.round(2)\n",
    "        output = rfm_df\n",
    "        #print(rfm_df[['Customer', 'RFM_Score']])\n",
    "        \n",
    "        rfm_df[\"Customer_segment\"] =np.where(rfm_df['RFM_Score'] >3.5, \"High Overdue - RFM>3.5\",\n",
    "                                    (np.where(rfm_df['RFM_Score'] > 2,\"Moderate Overdue - 2<RFM<3.5\",\n",
    "                                    np.where(rfm_df['RFM_Score'] > 1,'Less Overdue - 1<RFM<2', 'VeryLess Overdue - RFM<1'))))\n",
    "        \n",
    "        rfm_df[\"Customer_segment1\"] =np.where(rfm_df['RFM_Score'] >3.5, \"4\",\n",
    "                                    (np.where(rfm_df['RFM_Score'] > 2,\"3\",\n",
    "                                    np.where(rfm_df['RFM_Score'] > 1,'2', '1'))))\n",
    "        \n",
    "        #print(rfm_df[['Customer Name', 'RFM_Score', 'Customer_segment','Customer_segment1','M','Total Receivable']])\n",
    "        output = rfm_df\n",
    "        \n",
    "        \n",
    "        char = file_name.split('W')[1].split('.')[0]\n",
    "    \n",
    "        \n",
    "         #match = re.search(r'W(\\d+)\\.(xlsx|xlsm|xls)', os.path.basename(file_name))\n",
    "        \n",
    "                \n",
    "        submission = pd.DataFrame({\n",
    "        \"Date\": output[\"date\"],\n",
    "        \"Customer\": output[\"Customer Name\"],\n",
    "        \"RFM_Score\": output[\"RFM_Score\"],\n",
    "        \"Customer_segment\": output[\"Customer_segment\"],\n",
    "        #\"Week\":'W' + str(re.search(r'W(\\d+)\\.xlsx', os.path.basename(file_name)).group(1)),\n",
    "        \"Week\":'W' +  str(char),\n",
    "        \"Cluster\": output['Customer_segment1'],\n",
    "        \"Monetory\":output['M'],\n",
    "        \"Total Receivable\":output['Total Receivable']\n",
    "        })\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(char)\n",
    "        \n",
    "        output_file_name = os.path.join(output_folder, os.path.basename(file_name))\n",
    "        output_file_name = os.path.splitext(output_file_name)[0] + \" output.xlsx\"\n",
    "        \n",
    "        \n",
    "        submission = submission.replace('Week ', 'W', regex=True)\n",
    "        #print(submission.head(3))\n",
    "        \n",
    "        # Remove the time part (set it to midnight)\n",
    "        submission['Date'] = submission['Date'].dt.date\n",
    "        \n",
    "        submission.to_excel(output_file_name, index=False)\n",
    "        submission1 = pd.read_excel(output_file_name)\n",
    "        #print(submission1.head(5))\n",
    "\n",
    "        \n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3cae7799-2293-4920-98e1-04c94e75ddce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2022 W16 output.xlsx', '2022 W17 output.xlsx', '2022 W18 output.xlsx', '2022 W19 output.xlsx', '2022 W20 output.xlsx', '2022 W21 output.xlsx', '2022 W22 output.xlsx', '2022 W23 output.xlsx', '2022 W24 output.xlsx', '2022 W25 output.xlsx', '2022 W26 output.xlsx', '2022 W27 output.xlsx', '2022 W28 output.xlsx', '2022 W29 output.xlsx', '2022 W30 output.xlsx', '2022 W31 output.xlsx', '2022 W32 output.xlsx', '2022 W33 output.xlsx', '2022 W34 output.xlsx', '2022 W35 output.xlsx', '2022 W36 output.xlsx', '2022 W37 output.xlsx', '2022 W38 output.xlsx', '2022 W39 output.xlsx', '2022 W40 output.xlsx', '2022 W41 output.xlsx', '2022 W42 output.xlsx', '2022 W43 output.xlsx', '2022 W44 output.xlsx', '2022 W45 output.xlsx', '2022 W46 output.xlsx', '2022 W47 output.xlsx', '2022 W48 output.xlsx', '2022 W49 output.xlsx', '2022 W50 output.xlsx', '2022 W51 output.xlsx', '2023 W1 output.xlsx', '2023 W10 output.xlsx', '2023 W11 output.xlsx', '2023 W12 output.xlsx', '2023 W13 output.xlsx', '2023 W14 output.xlsx', '2023 W15 output.xlsx', '2023 W16 output.xlsx', '2023 W17 output.xlsx', '2023 W18 output.xlsx', '2023 W19 output.xlsx', '2023 W2 output.xlsx', '2023 W20 output.xlsx', '2023 W21 output.xlsx', '2023 W3 output.xlsx', '2023 W4 output.xlsx', '2023 W5 output.xlsx', '2023 W6 output.xlsx', '2023 W7 output.xlsx', '2023 W8 output.xlsx', '2023 W9 output.xlsx']\n",
      "            Date                          Customer  RFM_Score  \\\n",
      "1350  2023-05-22                         Amazon UK   2.938596   \n",
      "1349  2023-05-22                             Aerie   3.337719   \n",
      "1346  2023-05-22  AMAZON ESSENTIAL - SPOTTED ZEBRA   2.730263   \n",
      "1348  2023-05-22   Abercrombie & Fitch Trading Co.   2.958333   \n",
      "1362  2023-05-22                    In Private inc   2.813596   \n",
      "\n",
      "                  Customer_segment Week  Cluster   Monetory  Total Receivable  \n",
      "1350  Moderate Overdue - 2<RFM<3.5  W21        3   47949.63          47949.63  \n",
      "1349  Moderate Overdue - 2<RFM<3.5  W21        3  212315.44        4516649.84  \n",
      "1346  Moderate Overdue - 2<RFM<3.5  W21        3   18273.80          18273.80  \n",
      "1348  Moderate Overdue - 2<RFM<3.5  W21        3   39162.85         319147.78  \n",
      "1362  Moderate Overdue - 2<RFM<3.5  W21        3  288537.36        2138806.11  \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#-------------------------create consolidate file which include all customer weekly data-(4th script)--------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "#C:\\Users\\GCV\\Documents\\MAS\\Project AR\\Testing\\weekly_output_data\n",
    "input_file_path = r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/weekly_output_data/\"\n",
    "output_file_path = r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/data_files/\"\n",
    "\n",
    "#create a list to store all the file references of the input folder using the listdir function from the os library.\n",
    "\n",
    "excel_file_list = os.listdir(input_file_path)\n",
    "\n",
    "print(excel_file_list)\n",
    "\n",
    "\n",
    "#Once each file opens, use the append function to start consolidating the data stored in multiple files\n",
    "\n",
    "#create a new, blank dataframe, to handle the excel file imports\n",
    "df = pd.DataFrame()\n",
    "\n",
    "\n",
    "#Run a for loop to loop through each file in the list\n",
    "for excel_files in excel_file_list:\n",
    " #check for .xlsx suffix files only\n",
    " if excel_files.endswith((\".xlsx\", \".xls\", \".xlsm\")):\n",
    " #create a new dataframe to read/open each Excel file from the list of files created above\n",
    "    df1 = pd.read_excel(input_file_path + excel_files)\n",
    "     #print(df1.head())\n",
    "        \n",
    "     #append each file into the original empty dataframe\n",
    "    df= pd.concat([df, df1], ignore_index=True)\n",
    "        \n",
    "#print(df.head())\n",
    "df['Customer'] = df['Customer'].str.replace('AMAZON ESSENTIAL / SPOTTED ZEBRA', 'AMAZON ESSENTIAL - SPOTTED ZEBRA')\n",
    "df['Date'] = df['Date'].dt.date\n",
    "#transfer final output to an Excel (xlsx) file on the output path \n",
    "\n",
    "df_sorted = df.sort_values('Date',ascending = True)\n",
    "df_sorted.to_excel(output_file_path+\"Consolidated_file_sorted.xlsx\")\n",
    "\n",
    "print(df_sorted.tail())\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07f2e3f3-030f-4053-a500-e598fc678b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the same 'Consolidated_file_sorted.xlsx' where use in cluster prediction python file amd start to build monetory_pbi_final.xlsx file\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Read the input file\n",
    "#consolidate_df = pd.read_excel(r'C:\\Users\\GCV\\Documents\\MAS\\Project AR\\2nd_final_output\\Consolidated_file.xlsx\")\n",
    "consolidate_df = pd.read_excel(r'C:\\Users\\GCV\\Documents\\MAS\\Project AR\\Testing\\Demo\\data_files\\Consolidated_file_sorted.xlsx')\n",
    "\n",
    "\n",
    "# Get unique customer names\n",
    "customer_names = consolidate_df[\"Customer\"].unique()\n",
    "\n",
    "# Iterate over each customer and create a separate Excel file\n",
    "for customer in customer_names:\n",
    "    # Filter the dataframe for the current customer\n",
    "    #customer_df = consolidate_df[consolidate_df[\"Customer\"] == customer][[\"RFM_Score\", \"Customer_segment\", \"Week\", \"Cluster\",\"Monetory\"]]\n",
    "    customer_df1 = consolidate_df[consolidate_df[\"Customer\"] == customer][[ \"Date\",\"Week\",\"Monetory\",\"Total Receivable\"]]\n",
    "\n",
    "    # Create output file name\n",
    "    output_file_name = f\"{customer}.xlsx\"\n",
    "    #output_folder = r\"C:/Users/GCV/Documents/MAS/Project AR/con_output/\"\n",
    "    output_folder1 = r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/monetory_cust_RFM/\"\n",
    "   \n",
    "    #output_file_path = os.path.join(output_folder, output_file_name)\n",
    "    output_file_path1 = os.path.join(output_folder1, output_file_name)\n",
    "\n",
    "    #print(output_folder)\n",
    "    #print(output_file_name)\n",
    "    #print(output_file_path)\n",
    "\n",
    "    # Save the filtered dataframe to a separate Excel file\n",
    "    #customer_df.to_excel(output_file_path, index=False)\n",
    "    \n",
    "     #remove time part of date\n",
    "    customer_df1['Date'] = customer_df1['Date'].dt.date\n",
    "    \n",
    "    #Remove customers who has low data points(when customer has low datapoints ES model occurs some errors)\n",
    "    row_count  = customer_df1.shape[0]\n",
    "    if row_count >20:\n",
    "        customer_df1.to_excel(output_file_path1, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "acd5c7da-3424-42d6-9d0b-c617485e2e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # Read the input file\n",
    "# #consolidate_df = pd.read_excel(r'C:\\Users\\GCV\\Documents\\MAS\\Project AR\\2nd_final_output\\Consolidated_file.xlsx\")\n",
    "# output_file_path = r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/data_files/\"\n",
    "\n",
    "# consolidate_df = pd.read_excel(r'C:\\Users\\GCV\\Documents\\MAS\\Project AR\\Testing\\Demo\\data_files\\Consolidated_file_sorted.xlsx')\n",
    "\n",
    "# df_pivot = consolidate_df.pivot(index='Date', columns='Customer', values='Monetory')\n",
    "# # df_pivot.columns\n",
    "\n",
    "# # #df = df.rename(columns={'OldColumnName': 'NewColumnName'})\n",
    "\n",
    "# # #\n",
    "# # Calculate the count of non-null values in each column\n",
    "# non_null_counts = df_pivot.count()\n",
    "# # print(non_null_counts)\n",
    "# # print(type(non_null_counts))\n",
    "# # # Filter the column names based on the count\n",
    "# selected_columns = non_null_counts[non_null_counts >= 20].index\n",
    "# print(selected_columns)\n",
    "# # # Delete the selected columns\n",
    "# df_pivot = df_pivot[selected_columns]\n",
    "# df_pivot.columns\n",
    "# df_pivot = df_pivot.fillna(0)\n",
    "# #df_pivot = df_pivot.index.rename('Weeks')\n",
    "# print(df_pivot.columns)\n",
    "# print(df_pivot.head())\n",
    "# df_pivot.to_excel(output_file_path+\"input_data_sorted_1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f99d7706-3618-4ed3-aa5f-bf175adcf790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# output_file_path = r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/data_files/\"\n",
    "\n",
    "# # Load the Excel file into a DataFrame\n",
    "# df = pd.read_excel(r'C:\\Users\\GCV\\Documents\\MAS\\Project AR\\Testing\\Demo\\data_files\\input_data_sorted_1.xlsx')\n",
    "\n",
    "# # Specify the column containing customer names\n",
    "# customer_name_column = df.columns[1:]\n",
    "\n",
    "# # List to store customer names with more than 10 zeros\n",
    "# customers_to_remove = []\n",
    "\n",
    "# # Iterate over customer names\n",
    "# for customer in df[customer_name_column]:\n",
    "#     # Get the column name corresponding to the customer\n",
    "#     column_name = customer\n",
    "    \n",
    "#     # Get the count of zeros in the column\n",
    "#     zero_count = (df[column_name] == 0).sum()\n",
    "    \n",
    "#     # Check if the column has more than 10 zeros\n",
    "#     if zero_count > 20:\n",
    "#         customers_to_remove.append(column_name)\n",
    "\n",
    "# # Filter out the customers with more than 10 zeros from the DataFrame\n",
    "# df = df.drop(columns=customers_to_remove)\n",
    "\n",
    "# # Check if 'Unnamed: 0' column exists in the DataFrame\n",
    "# if 'Unnamed: 0' in df.columns:\n",
    "#     # Remove the 'Unnamed: 0' column\n",
    "#     df = df.drop('Unnamed: 0', axis=1)\n",
    "# #print(columns)\n",
    "# # Print the updated DataFrame\n",
    "# print(df.columns)\n",
    "\n",
    "# df.to_excel(output_file_path+\"model_input_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3365385-3bb4-42f2-bcab-e0023c27f9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# # Read the input file\n",
    "\n",
    "# df_new = pd.read_excel(r'C:\\Users\\GCV\\Documents\\MAS\\Project AR\\Testing\\Demo\\data_files\\model_input_data.xlsx')\n",
    "\n",
    "# df_new = df_new.rename(columns={'Date': 'Weeks'})\n",
    "\n",
    "# columns = df_new.columns\n",
    "# #print(columns)\n",
    "\n",
    "\n",
    "# if 'Unnamed: 0'in df_new.columns:\n",
    "#     df_new = df_new.drop('Unnamed: 0', axis=1)\n",
    "# print(df_new.columns)\n",
    "\n",
    "# # # Convert the index to a Numpy array\n",
    "# index_array = df_new.columns.to_numpy()\n",
    "# customer_name = np.delete(index_array, 0)\n",
    "# print(customer_name)\n",
    "# # test_customers = ['Aerie','Abercrombie & Fitch Trading Co.','Athleta']\n",
    "# # print(test_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8f9d58d-8a6c-4347-8e8b-ca7f2d08f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "795c217f-fe01-4cdb-ba67-466ae8eb02c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# folder = r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/model_output_data/\"\n",
    "# smape_list = []\n",
    "# for i in customer_name:\n",
    "#     print(\"--------------------NEW CUSTOMER-----------------------\")\n",
    "#     df=df_new[[\"Weeks\" , i ]]\n",
    "#     #df.info()\n",
    "\n",
    "   \n",
    "    \n",
    "#     df[\"Weeks\"] = pd.to_datetime(df[\"Weeks\"])\n",
    "#     df.set_index(['Weeks'],inplace = True)\n",
    "#     #df.info()\n",
    "#     #print(df.isnull().sum())\n",
    "    \n",
    "    \n",
    "# #     plt.figure(figsize=(12,6))\n",
    "# #     plt.plot(df[i])\n",
    "# #     plt.title(\"Customers\")\n",
    "# #     plt.ylabel(\"i\")\n",
    "# #     plt.xlabel(\"Weeks\")\n",
    "# #     plt.show()\n",
    "    \n",
    "    \n",
    "# #     plt.figure(figsize=(12,5))\n",
    "# #     plt.plot(df[i])\n",
    "# #     plt.title(\"Customers\")\n",
    "# #     plt.ylabel(i)\n",
    "# #     plt.xlabel(\"Weeks\")\n",
    "# #     for x in df.index[df.index.week==52]:\n",
    "# #      plt.axvline(x=x, color='red');\n",
    "# #     plt.show();\n",
    "    \n",
    "    \n",
    "    \n",
    "# #     from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# #     result = seasonal_decompose(df[i], model='additive')\n",
    "# #     fig, axs = plt.subplots(2, 2,figsize=(15,8))\n",
    "# #     axs[0, 0].plot(result.observed)\n",
    "# #     axs[0, 0].autoscale(axis='x',tight=True)\n",
    "# #     axs[0, 0].set_title('Observed')\n",
    "# #     axs[0, 1].plot(result.trend,'tab:orange')\n",
    "# #     axs[0, 1].autoscale(axis='x',tight=True)\n",
    "# #     axs[0, 1].set_title('Trend')\n",
    "# #     axs[1, 0].plot(result.seasonal, 'tab:green')\n",
    "# #     axs[1, 0].autoscale(axis='x',tight=True)\n",
    "# #     axs[1, 0].set_title('Seasonal')\n",
    "# #     axs[1, 1].plot(result.resid, 'tab:red')\n",
    "# #     axs[1, 1].autoscale(axis='x',tight=True)\n",
    "# #     axs[1, 1].set_title('Residuals')\n",
    "# #     plt.show()\n",
    "\n",
    "#     from statsmodels.tsa.stattools import adfuller\n",
    "#     def adf_test(df):\n",
    "#         result=adfuller(df)\n",
    "#         print(\"P Value: \",result[1])\n",
    "#         if result[1]<=0.05:\n",
    "#             print(\"Strong evidence aganist Null Hypothesis. So, reject Null Hypothesis and conclude data is stationary.\")\n",
    "#             return(True)\n",
    "#         else:\n",
    "#             print(\"Weak evidence aganist Null Hypothesis. So, accept Null Hypothesis and conclude data is non-stationary.\")\n",
    "#             return(False)\n",
    "#     adf_test(df)\n",
    "\n",
    "#     def convert_non_stationary_to_stationary(df):\n",
    "#         d=0\n",
    "#         new_df=df\n",
    "#         while True:\n",
    "#             new_df=new_df-new_df.shift()\n",
    "#             new_df.dropna(inplace=True)\n",
    "#             d=d+1\n",
    "#             if adf_test(new_df):\n",
    "#                 print(\"d-value is\",d)\n",
    "#                 break\n",
    "#     convert_non_stationary_to_stationary(df)\n",
    "\n",
    "#     train = df.iloc[:len(df)-3]\n",
    "#     test = df.iloc[len(df)-3:]\n",
    "#     print(\"----------------------------testing Values----------------------------------------------------\")\n",
    "#     print(f'testing values {test}')\n",
    "#     #print(test.index)    \n",
    "#     print(\"------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "#     from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "#     from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "#     from pmdarima import auto_arima\n",
    "#     #print(auto_arima(df[i],seasonal=False,m=52).summary())\n",
    "\n",
    "#     model = SARIMAX(train[i],order=(1, 1, 1),seasonal_order=(0, 1, [], 52))\n",
    "#     results = model.fit()\n",
    "#     #print(results.summary())\n",
    "\n",
    "#     start=len(train)\n",
    "    \n",
    "  \n",
    "#     end=len(train)+len(test)-1\n",
    "   \n",
    "#     predicted_values = results.predict(start=start, end=end)\n",
    "#     predicted_values.index = test.index\n",
    "\n",
    "#     print(\"#########################################################################################################\")\n",
    "#     predicted_vales_index = predicted_values.index\n",
    "#     print(f\"index of predicted_values_index {predicted_vales_index}\")\n",
    "#     testing_vales_index = test.index\n",
    "#     print(f\"index of testing_values_index {testing_vales_index}\")\n",
    "#     print(\"#########################################################################################################\")\n",
    "    \n",
    "    \n",
    "#     ax = test[i].plot(figsize=(12,5))\n",
    "#     predicted_values.plot()\n",
    "#     plt.legend()\n",
    "#     ax.autoscale(axis='x',tight=True)\n",
    "#     print(\"----------------------------predicted Values----------------------------------------------------\")\n",
    "#     print(predicted_values)\n",
    "#     print(\"-------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "#     def smape(a, f):\n",
    "#       return 1/len(a) * np.sum(2 * np.abs(f-a) / (np.abs(a) + np.abs(f)))*100\n",
    "\n",
    "#     smape=smape(test[i], predicted_values)\n",
    "#     print(f'SMAPE Values:----- {smape}')\n",
    "#     #smape_list.append((i, smape))\n",
    "    \n",
    "#     import sklearn as sk\n",
    "#     from sklearn.metrics import mean_squared_error\n",
    "#     from sklearn.metrics import mean_absolute_error\n",
    "#     from sklearn.metrics import mean_absolute_percentage_error\n",
    "#     print(\"mean_squared_error :\",mean_squared_error(test[i],predicted_values ))\n",
    "#     #print(\"root_mean_squared_error :\",mean_squared_error(test[f\" Item No {i}\"],predicted_values, squared=False))\n",
    "#     #print(\"mean_absolute_error :\",mean_absolute_error(test[f\" Item No {i}\"],predicted_values))\n",
    "#     print(\"mean_absolute_percentage_error :\",mean_absolute_percentage_error(test[i],predicted_values))\n",
    "\n",
    "#     model = SARIMAX(df[i],order=(1, 1, 1),seasonal_order=(0, 1, [], 52))\n",
    "#     results = model.fit()\n",
    "#     #print(results.summary())\n",
    "\n",
    "#     #future predictions\n",
    "#     predicted_values_future = results.predict(start=len(df), end=len(df)+3)\n",
    "#     #print(predicted_values_future)\n",
    "#     # df[i].plot(figsize=(12,6))\n",
    "#     # predicted_values_future.plot()\n",
    "#     # plt.legend()\n",
    "#     # plt.show()  \n",
    "#     print(\"----------------------------Future Prediction Values----------------------------------------------------\")\n",
    "#     print(f\" Customer - {i} Predicted Future Values are \\n\"  , predicted_values_future)\n",
    "#     print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "#     print(\"-----------------------------------------SMAPE-----------------------------------------------------------------\")\n",
    "#     print(f\"Customer - {i} sMAPE is :-- \" , smape)\n",
    "#     print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "#     print(\"-------------------------------------Predicted Values-----------------------------------------------------------------\")\n",
    "\n",
    "#     print(f\" Customer - {i} Predicted Values are \\n\" , predicted_values)\n",
    "#     print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    \n",
    "#     new_data = {'Customer': [i], 'Predicted Values': [predicted_values] ,'Future Predictions': [predicted_values_future] , 'sMAPE':[smape],'Mean Squared Error': [mean_squared_error(test[i],predicted_values )] ,'Mean Absolute Error': [mean_absolute_error(test[i],predicted_values)] , 'Mean Absolute Percentage Error': [mean_absolute_percentage_error(test[i],predicted_values)]}  \n",
    "  \n",
    "#     final_data = pd.DataFrame(new_data)  \n",
    "#     file_name = f'{i}.xlsx'  \n",
    "#     file_path = os.path.join(folder, file_name)  \n",
    "#     # Print the output.   \n",
    "#     final_data.to_excel(file_path, index=False)\n",
    "      \n",
    "# # df = pd.DataFrame(smape_list, columns=['Customer', 'SMAPE'])\n",
    "# # print(smape_list)\n",
    "# # Save the dataframe to an Excel file\n",
    "# #df.to_excel(r'C:\\Users\\GCV\\Documents\\MAS\\Project AR\\Testing\\Demo\\customer_smape.xlsx', index=False)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3cd08580-6244-4791-90a3-812320956983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # Read the input file\n",
    "# #consolidate_df = pd.read_excel(r'C:\\Users\\GCV\\Documents\\MAS\\Project AR\\2nd_final_output\\Consolidated_file.xlsx\")\n",
    "# output_file_path = r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/data_files/\"\n",
    "\n",
    "# consolidate_df = pd.read_excel(r'C:\\Users\\GCV\\Documents\\MAS\\Project AR\\Testing\\Demo\\data_files\\Consolidated_file_sorted.xlsx')\n",
    "\n",
    "# df_pivot = consolidate_df.pivot(index='Date', columns='Customer', values='Monetory')\n",
    "# df_pivot.head()\n",
    "# df_pivot.to_excel(output_file_path+\"input_data_sorted.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5e0c0c0-e410-43be-9d7c-593462deb00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing\n",
    "from openpyxl import Workbook as wb\n",
    "from itertools import cycle\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c84b4366-79dc-44f9-9da8-8dfb32054276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put model into future_prediction function\n",
    "def future_prediction(filename):\n",
    "    # Load the input data from the excel file\n",
    "\n",
    "    # Perform the time series modeling here\n",
    "\n",
    "    %matplotlib inline \n",
    "    plt.style.use('bmh')\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.rc('xtick', labelsize=15) \n",
    "    plt.rc('ytick', labelsize=15) \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    color_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    color_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "    \n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    #calculate smape\n",
    "    def smape(a,f):\n",
    "        return 1/len(a)*np.sum(2 * np.abs(f - a) / (np.abs(a)+ np.abs(f))*100)\n",
    "    \n",
    "    #read the file\n",
    "    rfm_data=pd.read_excel(filename)\n",
    "    rfm_data.head()\n",
    "    #split data into train and test\n",
    "    row_count  = rfm_data.shape[0]\n",
    "\n",
    "    train_data = rfm_data.iloc[:(row_count-8)]\n",
    "    test_data = rfm_data.iloc[(row_count-8):] \n",
    "    \n",
    "    # print(\"##########################################################################################################\")\n",
    "    # print(base_name)\n",
    "    # #print(train_data)\n",
    "    # print(\"++++++++++++++++++++++++++++++\")\n",
    "    # print(f\"test data:\\n{test_data}\")\n",
    "    \n",
    "    #print(train_data)\n",
    "    #getting predicting values\n",
    "    span = 12\n",
    "    alpha = 2/(span+1)\n",
    "\n",
    "    simpleExpSmooth_model = SimpleExpSmoothing(train_data['Monetory']).fit(smoothing_level=alpha,optimized=True)\n",
    "    # i change following seasonal period 6 to 8\n",
    "    doubleExpSmooth_model = ExponentialSmoothing(train_data['Monetory'],trend='add',seasonal_periods=8).fit()\n",
    "    tripleExpSmooth_model = ExponentialSmoothing(train_data['Monetory'],trend='add',damped_trend=True,seasonal='add',seasonal_periods=6).fit()\n",
    "    \n",
    "    predictions_simpleExpSmooth_model = simpleExpSmooth_model.forecast(8)\n",
    "    predictions_doubleExpSmooth_model = doubleExpSmooth_model.forecast(8)\n",
    "    predictions_tripleExpSmooth_model = tripleExpSmooth_model.forecast(8)\n",
    "    #print(predictions_tripleExpSmooth_model ,predictions_simpleExpSmooth_model , predictions_doubleExpSmooth_model )\n",
    "    \n",
    "    \n",
    "    #calculate smape\n",
    "    smape_simple = smape(test_data[\"Monetory\"] ,predictions_tripleExpSmooth_model)\n",
    "    smape_double = smape(test_data[\"Monetory\"] ,predictions_doubleExpSmooth_model)\n",
    "    smape_triple = smape(test_data[\"Monetory\"] ,predictions_tripleExpSmooth_model)\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_data['Monetory'].plot(legend=True,label='TRAIN')\n",
    "    test_data['Monetory'].plot(legend=True,label='TEST',figsize=(15,6))\n",
    "    predictions_simpleExpSmooth_model.plot(legend=True,label='Simple Exponential Forecast')\n",
    "    predictions_doubleExpSmooth_model.plot(legend=True,label='Double Exponential Forecast')\n",
    "    predictions_tripleExpSmooth_model.plot(legend=True,label='Triple Exponential Forecast')\n",
    "\n",
    "\n",
    "    model = tripleExpSmooth_model = ExponentialSmoothing(train_data['Monetory'],trend='add',damped_trend=True,seasonal='add',seasonal_periods=6).fit()\n",
    "    fcast_simple = simpleExpSmooth_model.predict(len(rfm_data),len(rfm_data)+ 3).rename('Simple Exponential Forecast')\n",
    "    fcast_double = doubleExpSmooth_model.predict(len(rfm_data),len(rfm_data)+3).rename('Double Exponential Forecast')\n",
    "    fcast_triple = tripleExpSmooth_model.predict(len(rfm_data),len(rfm_data)+3).rename('Triple Exponential Forecast')\n",
    "\n",
    "#     print(\"*********************************\")\n",
    "#     print(f\"predicted values: \\n{predictions_tripleExpSmooth_model}\")\n",
    "#     print(\"##################################\")\n",
    "#     print(f\"sMAPE tripple:- {smape_triple}\")\n",
    "#     print(f\"sMAPE double:- {smape_double}\")\n",
    "#     print(f\"sMAPE simple:- {smape_simple}\")\n",
    "#     print(type(test_data))\n",
    "#     print(type(predictions_tripleExpSmooth_model))\n",
    "    \n",
    "#     print(\"------------------------------------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    \n",
    "    week_df = test_data[['Week']].values\n",
    "    #week_df.columns = ['']\n",
    "    s = week_df.squeeze()\n",
    "    w_series = pd.Series(s)\n",
    "\n",
    "    new_data = {'Customer Name': [filename],'Predicted Weeks': [w_series],'Predicted Values': [predictions_tripleExpSmooth_model] ,'Future Predictions Simple ES': [fcast_simple] , 'Future Predictions Double ES': [fcast_double], 'Future Predictions Triple ES': [fcast_double], 'sMAPE Simple ES':[smape_simple], 'sMAPE Double ES':[smape_double], 'sMAPE Triple ES':[smape_triple]}  \n",
    "  \n",
    "    # Create DataFrame  \n",
    "    final_data = pd.DataFrame(new_data)  \n",
    "        \n",
    "\n",
    "    return final_data  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e2e971e-15a5-4116-bc3d-349331eedd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction part done\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "input_folder = \"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/monetory_cust_RFM/\"\n",
    "output_folder = \"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/monetory_cust_prediction/\"\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".xlsx\"):\n",
    "        #print(filename)\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        final_data = future_prediction(file_path)\n",
    "\n",
    "        \n",
    "        final_data.to_excel(os.path.join(output_folder, filename))\n",
    "\n",
    "        #print(file_path)\n",
    "        #print(final_data)\n",
    "print(\"prediction part done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "498b2260-3de6-4cbf-8017-d6dbd02cd8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Customer', 'Week', 'Week_num', '2022 Week', '2023 Week',\n",
      "       'Monetary_2022', 'Total_Receivable_2022', 'Date_2022', 'Monetary_2023',\n",
      "       'Total_Receivable_2023', 'Date_2023'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#creating dashboard back end excel file(adding 2022(rfm,cluster,segmentation) and 2023(rfm,cluster,segmentation) columns)\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "# read in the data from an Excel file\n",
    "df = pd.read_excel(r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/data_files/Consolidated_file_sorted.xlsx\")\n",
    "\n",
    "# extract week numbers as integers\n",
    "df['Week_num'] = df['Week'].str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------\n",
    "# Extract the year from the 'Date' column\n",
    "df['Year'] = pd.DatetimeIndex(df['Date']).year\n",
    "\n",
    "# Filter the data frame based on the year\n",
    "df_2022 = df[df['Year'] == 2022] \n",
    "df_2023 = df[df['Year'] == 2023]\n",
    "\n",
    "# Drop the 'Year' column from both data frames\n",
    "df_2022.drop('Year', axis=1, inplace=True)\n",
    "df_2023.drop('Year', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# create new columns for 2022 and 2023 weeks\n",
    "df_2022['2022 Week'] = df_2022['Week']\n",
    "df_2023['2023 Week'] = df_2023['Week']\n",
    "\n",
    "\n",
    "# concatenate the two subsets back into the original DataFrame\n",
    "df = pd.concat([df_2022, df_2023], ignore_index=True)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create new columns with empty values\n",
    "df['Monetary_2022'] = ''\n",
    "df['Total_Receivable_2022'] = ''\n",
    "df['Date_2022'] = ''\n",
    "\n",
    "\n",
    "\n",
    "# Create a boolean mask for the rows where \"2022 Week\" is not null\n",
    "mask = ~df['2022 Week'].isnull()\n",
    "\n",
    "# Assign the values to the new columns using the mask\n",
    "df.loc[mask, 'Monetary_2022'] = df.loc[mask, 'Monetory']\n",
    "df.loc[mask, 'Total_Receivable_2022'] = df.loc[mask, 'Total Receivable']\n",
    "df.loc[mask, 'Date_2022'] = df.loc[mask, 'Date']\n",
    "\n",
    "\n",
    "df['Monetary_2023'] = ''\n",
    "df['Total_Receivable_2023'] = ''\n",
    "df['Date_2023'] = ''\n",
    "\n",
    "# Create a boolean mask for the rows where \"2022 Week\" is not null\n",
    "mask = ~df['2023 Week'].isnull()\n",
    "\n",
    "# Assign the values to the new columns using the mask\n",
    "df.loc[mask, 'Monetary_2023'] = df.loc[mask, 'Monetory']\n",
    "df.loc[mask, 'Total_Receivable_2023'] = df.loc[mask, 'Total Receivable']\n",
    "df.loc[mask, 'Date_2023'] = df.loc[mask, 'Date']\n",
    "\n",
    "df = df.drop(columns=['Unnamed: 0','Total Receivable', 'RFM_Score', 'Customer_segment','Cluster','Monetory'])#removed monetary column also\n",
    "\n",
    "\n",
    "\n",
    "print(df.columns)\n",
    "# save the result to a new Excel file\n",
    "df.to_excel(r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/data_files/Monetory_Actual_values.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "45d4bbcd-c93a-44f0-aca5-cc911392a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split multivalues in cells to columns\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def process_input_file(file_path):\n",
    "    # read the input file as a DataFrame with temporary header names\n",
    "    df = pd.read_excel(file_path, header=0)\n",
    "    df1= df[['Predicted Values','Predicted Weeks', 'Future Predictions Simple ES','Future Predictions Double ES', 'Future Predictions Triple ES']]\n",
    "    df1 = df1.fillna('')\n",
    "    df_sMAPE =df[['sMAPE Simple ES','sMAPE Double ES','sMAPE Triple ES']]\n",
    "    # set temporary header names\n",
    "    temp_header = [\"Column\" + str(i) for i in range(df1.shape[1])]\n",
    "    df1.columns = temp_header\n",
    "    \n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "    for col in df1.columns:\n",
    "        # split the 'Actual Values' column into separate columns\n",
    "        df_actual = df1[col].str.split('\\n', expand=True)\n",
    "        df_actual = df_actual.transpose()\n",
    "        df_actual = df_actual.drop(index=(df_actual.shape[0]-1))\n",
    "        \n",
    "        new_df = pd.concat([new_df, df_actual], axis=1, ignore_index=False)\n",
    "    \n",
    "    # reset header names to None\n",
    "    #new_df.columns = None\n",
    "    new_df.columns = ['Predicted Values','Predicted Weeks', 'Future Predictions Simple ES','Future Predictions Double ES', 'Future Predictions Triple ES']\n",
    "\n",
    "    # reset index\n",
    "    new_df = new_df.reset_index(drop=True)\n",
    "    #split multi line text in single values to separate columns\n",
    "    new_df = new_df.apply(lambda x: x.str.split())\n",
    "\n",
    "    # extract the second value from each cell in all columns(ex- 33 2.321 to 2.321)\n",
    "    new_df = new_df.apply(lambda x: x.str[1])\n",
    "    \n",
    "    #file_name = os.path.basename(file_path)\n",
    "    file_name, file_extension = os.path.splitext(os.path.basename(file_path))\n",
    "\n",
    "    #file_name = file_path.split('.')[0]\n",
    "    # create a new column with the file name\n",
    "    new_df['customer'] = file_name\n",
    "\n",
    "    \n",
    "    \n",
    "    final_df = pd.concat([new_df, df_sMAPE], axis=1, ignore_index=False)\n",
    "    final_df = final_df[['customer','Predicted Values','Predicted Weeks', 'Future Predictions Simple ES','Future Predictions Double ES', 'Future Predictions Triple ES','sMAPE Simple ES', 'sMAPE Double ES', 'sMAPE Triple ES']]\n",
    "    final_df = final_df.fillna(0)\n",
    "    \n",
    "    #selecting minimum sMAPE and get best predicted values\n",
    "    pred_cols  = final_df[['Future Predictions Simple ES','Future Predictions Double ES', 'Future Predictions Triple ES']]\n",
    "    sMAPE_cols = final_df[['sMAPE Simple ES', 'sMAPE Double ES', 'sMAPE Triple ES']]\n",
    "   \n",
    "    #sort cloumn names in sMAPEs\n",
    "    sorted_columns = sMAPE_cols.mean().sort_values().index.tolist()\n",
    "    \n",
    "    \n",
    "    # create an empty list to store the column names\n",
    "    dup_col = []\n",
    "\n",
    "    # iterate over the columns\n",
    "    for col in pred_cols.columns:\n",
    "        # check if all values in the column are duplicates\n",
    "        if pred_cols[col].duplicated().all():\n",
    "            # add the column name to the list\n",
    "            dup_col.append(col)\n",
    "    \n",
    "       \n",
    "        for i, col in enumerate(sorted_columns):\n",
    "            \n",
    "            if col == 'sMAPE Triple ES':\n",
    "                if col in dup_col:\n",
    "                    break\n",
    "                else:\n",
    "                    best_col = 'Future Predictions Triple ES'\n",
    "            elif col == 'sMAPE Double ES':\n",
    "                if col in dup_col:\n",
    "                    break\n",
    "                else:\n",
    "                    best_col = 'Future Predictions Double ES'\n",
    "            else:\n",
    "                if col in dup_col:\n",
    "                    break\n",
    "                else:\n",
    "                    best_col = 'Future Predictions Simple ES'\n",
    "\n",
    "                    \n",
    "    final_df['best predict'] = final_df[best_col]  \n",
    "    smape =  sMAPE_cols[sorted_columns[0]]\n",
    "    final_df['sMAPE'] = smape\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9ab3c3e2-6eda-464e-b4f7-372c591f088b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#some of monetory predictions are negative here if nessary remove them\n",
    "\n",
    "#save each dfs in related folderr\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/model_output_data/\"\n",
    "input_folder = \"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/monetory_cust_prediction/\"\n",
    "output_folder = \"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/monetory_cust_prediction_data/\"\n",
    "\n",
    "# Set the file pattern you want to match, for example, all files with \".txt\" extension\n",
    "\n",
    "file_pattern= input_folder + \"*.xlsx\"\n",
    "\n",
    "# Use glob function to get the file names that match each file pattern\n",
    "file_names = glob.glob(file_pattern)\n",
    "\n",
    "# Loop through each file name and process the corresponding Excel file\n",
    "for file_name in file_names:\n",
    "    final_data = process_input_file(file_name)\n",
    "    #final_data2 = add_week_column(file_name)\n",
    "   \n",
    "    #final_data = pd.concat([final_data2,final_data1], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    output_file_name = os.path.join(output_folder, os.path.basename(file_name))\n",
    "    output_file_name = os.path.splitext(output_file_name)[0] + \".xlsx\"\n",
    "    final_data.to_excel(output_file_name, index = False)    \n",
    "    \n",
    "    \n",
    "print(\"done\") #some of monetory predictions are negative here if nessary remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5df1b4ee-dbf1-4d00-82ff-9a3dd64b3df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#duplicate code\n",
    "#make data frames related to each customer with prediction ,smape,...\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "input_folder = \"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/monetory_cust_prediction_data/\"\n",
    "output_folder= \"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/monetory_cust_final_df/\"\n",
    "\n",
    "# Set the file pattern want to match\n",
    "\n",
    "file_pattern= input_folder + \"*.xlsx\"\n",
    "\n",
    "# Use glob function to get the file names that match each file pattern\n",
    "file_names = glob.glob(file_pattern)\n",
    "\n",
    "#print(file_names)\n",
    "for file in file_names:\n",
    "    df = pd.read_excel(file)\n",
    "\n",
    "    df_con = pd.read_excel(r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/data_files/Monetory_Actual_values.xlsx\")\n",
    "\n",
    "\n",
    "    first_value = df_con['Week_num'].iloc[1]\n",
    "    last_value = df_con['Week_num'].iloc[-1]\n",
    "\n",
    "    # calculate the starting week number\n",
    "    starting_week = last_value -8 + 52\n",
    "   \n",
    "\n",
    "    # define the range of values to repeat\n",
    "    start_val = 1\n",
    "    end_val = 51\n",
    "    repeat_val = starting_week\n",
    "\n",
    "    # calculate the number of times to repeat the range\n",
    "    num_rows =  len(df.index)\n",
    "    num_repeats = int(np.ceil(num_rows / (end_val - start_val + 1)))\n",
    "\n",
    "    # create the array of values to repeat\n",
    "    vals_to_repeat = np.concatenate((np.arange(start_val, end_val+1),np.arange(repeat_val, end_val+1)))\n",
    "\n",
    "    # repeat the array the necessary number of times\n",
    "    repeated_vals = np.tile(vals_to_repeat, num_repeats)[:num_rows]\n",
    "    # add the repeated values to the new column in the dataframe\n",
    "    week_vals = [(starting_week + i) % 51 or 51 for i in range(num_rows)]\n",
    "    df.insert(loc=0, column='Week', value=week_vals)\n",
    "    last_w = df['Week'].iloc[-1]\n",
    "    list_week = []\n",
    "    for i in range (4):\n",
    "        list_week.append(last_w + i+1)\n",
    "\n",
    "    df_w = pd.DataFrame({'Week': list_week})\n",
    "    \n",
    "    \n",
    "    # create a new DataFrame with the non-null values of the 'week' column\n",
    "    df_new = pd.DataFrame(df['Predicted Weeks'].dropna())\n",
    "\n",
    "    #print(df_new)\n",
    "    #df = pd.concat([df,df_w], axis =0).reset_index(drop = True)\n",
    "    df_w['Week'] = 'W' + df_w['Week'].astype(str)\n",
    "    df_w['Predicted Weeks'] =df_w['Week']\n",
    "    df_w = df_w.drop('Week', axis=1)\n",
    "    df_new = pd.concat([df_new,df_w], axis =0).reset_index(drop = True)\n",
    "    #print(df_new)\n",
    "    \n",
    "    df['Week num'] = df_new['Predicted Weeks']\n",
    "    #print(df_new['Predicted Weeks'])\n",
    "    \n",
    "    \n",
    "    new_cust = pd.DataFrame()\n",
    "    new_cust['Week num'] = df_new['Predicted Weeks']\n",
    "    new_cust['Week'] = new_cust['Week num'].str.replace('W', '').astype(int)\n",
    "    customer_name = df['customer'].iloc[1]\n",
    "\n",
    "    # Replace the missing values in the \"customer\" column with the customer name\n",
    "    new_cust['customer'] = customer_name\n",
    "    # print(customer_name)\n",
    "    # print(df)\n",
    "    new_cust['Predicted Values'] = df['Predicted Values']\n",
    "    new_cust['Future_prediction'] = ''\n",
    "    new_cust.loc[8:, 'Future_prediction'] = df.loc[:3, 'best predict'].values\n",
    "    #df.loc[8:, 'Future_prediction'] = df.loc[:3, 'best predict'].values.tolist() + [\"\"] * (len(df) - 8)\n",
    "    smape = df['sMAPE'][0]\n",
    "    new_cust['sMAPE'] =smape #df['sMAPE'].fillna(df['sMAPE'][0], inplace=True)\n",
    "    \n",
    "    new_cust = new_cust.fillna(0)\n",
    "    output_file_name = os.path.join(output_folder, os.path.basename(file))\n",
    "    output_file_name = os.path.splitext(output_file_name)[0] + \".xlsx\"\n",
    "    #print(new_cust)\n",
    "    \n",
    "    new_cust.to_excel(output_file_name, index = False)   \n",
    "    \n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b31f53bf-a9d1-4af8-8a3a-bbc3fe50205e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Week num', 'Week', 'customer', 'Predicted Values', 'Future_prediction',\n",
      "       'sMAPE'],\n",
      "      dtype='object')\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#merge all customer vise prediction files into one file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "input_folder =  \"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/monetory_cust_final_df/\"\n",
    "output_file = \"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/monetory_merged_predictions.xlsx\"\n",
    "\n",
    "# Set the file pattern you want to match, for example, all files with \".txt\" extension\n",
    "\n",
    "file_pattern= input_folder + \"*.xlsx\"\n",
    "\n",
    "# Use glob function to get the file names that match each file pattern\n",
    "file_names = glob.glob(file_pattern)\n",
    "\n",
    "\n",
    "# create empty dataframe to store merged data\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# loop through customer files and merge data\n",
    "for filename in file_names:   # replace file_list with the list of file names in your folder\n",
    "    df = pd.read_excel(filename)   # read in each file as a data frame\n",
    "    merged_df = pd.concat([merged_df, df], ignore_index=True)   # append each data frame to the merged data frame\n",
    "\n",
    "    \n",
    "    \n",
    "merged_df['Week'] , merged_df['Week num'] = merged_df['Week num'], merged_df['Week']\n",
    "# print the merged data frame\n",
    "print(merged_df.columns)\n",
    "\n",
    "\n",
    "# drop rows with negative values in \"future_prediction\" column\n",
    "merged_df = merged_df.drop(merged_df[merged_df['Future_prediction'] < 0].index)\n",
    "\n",
    "merged_df.to_excel(output_file)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "facabda3-efc1-4240-86ac-b48af079d365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "    Week                          customer  Week num  Future_prediction  \\\n",
      "8    W22   Abercrombie & Fitch Trading Co.        22       3.111987e+05   \n",
      "20   W22                             Aerie        22       2.692807e+06   \n",
      "32   W22  AMAZON ESSENTIAL - SPOTTED ZEBRA        22       2.042116e+04   \n",
      "44   W22                         Amazon UK        22       7.327682e+04   \n",
      "56   W22    AMERICAN DESIGNER FASHION, S.A        22       6.644386e+03   \n",
      "..   ...                               ...       ...                ...   \n",
      "289  W25                 Puritas (Pvt) Ltd        25       5.022088e+04   \n",
      "301  W25                  PVH ASIA LIMITED        25       4.816657e+04   \n",
      "321  W25                  Summersalt, Inc.        25       1.574864e+06   \n",
      "333  W25   The Colombo Fashion WSwim Pvt L        25       1.997185e+03   \n",
      "345  W25             Urban Island and FMLK        25       0.000000e+00   \n",
      "\n",
      "          sMAPE  \n",
      "8     47.207901  \n",
      "20   106.592320  \n",
      "32     0.660268  \n",
      "44    23.133443  \n",
      "56    42.483658  \n",
      "..          ...  \n",
      "289  129.062996  \n",
      "301  109.032183  \n",
      "321   17.037533  \n",
      "333   30.856579  \n",
      "345    0.000000  \n",
      "\n",
      "[90 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#getting filter values based on weeek number in predictions\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/monetory_merged_predictions.xlsx\")\n",
    "#df.head()\n",
    "\n",
    "df_con = pd.read_excel(r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/data_files/Monetory_Actual_values.xlsx\")\n",
    "\n",
    "\n",
    "first_value = df_con['Week_num'].iloc[1]\n",
    "last_value = df_con['Week_num'].iloc[-1]\n",
    "\n",
    "# calculate the starting week number\n",
    "starting_week = last_value -6 + 52\n",
    "\n",
    "\n",
    "start_val = 1\n",
    "end_val = 51\n",
    "repeat_val = starting_week\n",
    "\n",
    "# calculate the number of times to repeat the range\n",
    "num_rows =  len(df.index)\n",
    "num_repeats = int(np.ceil(num_rows / (end_val - start_val + 1)))\n",
    "\n",
    "# create the array of values to repeat\n",
    "vals_to_repeat = np.concatenate((np.arange(start_val, end_val+1),np.arange(repeat_val, end_val+1)))\n",
    "\n",
    "# repeat the array the necessary number of times\n",
    "repeated_vals = np.tile(vals_to_repeat, num_repeats)[:num_rows]\n",
    "# add the repeated values to the new column in the dataframe\n",
    "week_vals = [(starting_week + i) % 51 or 51 for i in range(num_rows)]\n",
    "\n",
    "pred_starting_week = (starting_week+6) % 51\n",
    "\n",
    "print(pred_starting_week)\n",
    "week_filter = [0]*4\n",
    "for i in range(4):\n",
    "    week_filter[i]= pred_starting_week +i\n",
    "\n",
    "    \n",
    "#print(week_filter)\n",
    "\n",
    "# create an empty list to store the filtered DataFrames\n",
    "df_list = []\n",
    "\n",
    "# loop over each value in week_list and filter the DataFrame\n",
    "for week in week_filter:\n",
    "    df_filtered = df[df['Week num'] == week]\n",
    "    df_list.append(df_filtered)\n",
    "\n",
    "# concatenate all the DataFrames in df_list into one DataFrame\n",
    "df_combined = pd.concat(df_list)\n",
    "\n",
    "\n",
    "# convert Future_prediction column to float\n",
    "df_combined['Future_prediction'] = df_combined['Future_prediction'].astype(float)\n",
    "    \n",
    "\n",
    "# # apply the function to create a new column\n",
    "# df_combined['Predicted_Customer_Segmentation'] = df_combined['Future_prediction'].apply(map_to_segment)\n",
    "# df_combined['Predicted_Customer_Cluster'] = df_combined['Future_prediction'].apply(map_to_cluster)\n",
    "\n",
    "df_to_actual = df_combined[['Week', 'customer','Week num', 'Future_prediction','sMAPE']]\n",
    "\n",
    "\n",
    "print(df_to_actual)\n",
    "\n",
    "df_to_actual.to_excel(r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/monetory_prediction_values_merge.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eef1b4b6-af24-43b4-b028-877292a15080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Customer', 'Week', 'Week_num', '2022 Week', '2023 Week',\n",
      "       'Monetary_2022', 'Total_Receivable_2022', 'Date_2022', 'Monetary_2023',\n",
      "       'Total_Receivable_2023', 'Date_2023', 'Future_prediction', 'sMAPE'],\n",
      "      dtype='object')\n",
      "Index(['Week', 'Customer', 'Week_num', 'Future_prediction', 'sMAPE',\n",
      "       'Monetory', '2022 Week', '2023 Week', 'Monetary_2022', 'Monetary_2023',\n",
      "       'Total_Receivable_2022', 'Total_Receivable_2023'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#concat Actual and prediction data frames in to one df\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "df_actual = pd.read_excel(r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/data_files/Monetory_Actual_values.xlsx\")\n",
    "df_predict = pd.read_excel(r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/monetory_prediction_values_merge.xlsx\")\n",
    "#df_reference =  pd.read_excel(r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Nature.xlsx\")\n",
    "\n",
    "\n",
    "df_actual['Future_prediction'] = ''\n",
    "df_actual['sMAPE'] = ''\n",
    "\n",
    "# df_actual['Predicted_Customer_Segmentation'] = ''\n",
    "# df_actual['Predicted_Customer_Cluster'] = ''\n",
    "\n",
    "# #chechk always if Unnamed: 0 column exists if its availble drop it(if it availble its harder to concat dfs)\n",
    "# if 'Unnamed: 0' in df.columns:\n",
    "#     df_actual.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "df_predict.rename(columns={'customer': 'Customer'}, inplace=True)\n",
    "df_predict.rename(columns={'Week num': 'Week_num'}, inplace=True)\n",
    "\n",
    "\n",
    "# ['Customer', 'Week', 'Monetory', 'Week_num', '2022 Week', '2023 Week','Monetary_2022', 'Monetary_2023'],\n",
    "\n",
    "# Index(['Week', 'customer', 'Week num', 'Future_prediction', 'sMAPE'], dtype='object')\n",
    "\n",
    "\n",
    "\n",
    "# creating null column because of concat\n",
    "df_predict[['Monetory', '2022 Week', '2023 Week', 'Monetary_2022', 'Monetary_2023','Total_Receivable_2022','Total_Receivable_2023']] = None\n",
    "\n",
    "print(df_actual.columns)\n",
    "print(df_predict.columns)\n",
    "\n",
    "\n",
    "#common_cols = list(set(df_actual.columns) & set(df_predict.columns))\n",
    "df_actual = pd.concat([df_actual, df_predict], axis=0).reset_index()\n",
    "\n",
    "\n",
    "#df_merged = pd.merge(df_actual, df_reference, on='Customer', how='left')\n",
    "\n",
    "df_actual.to_excel(r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/monetory_pbi_file_1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ae83ea6-b1cf-4cde-97de-dce943dfa444",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter each cusromer and get all data in single data frame \n",
    "\n",
    "#consolidate_df = pd.read_excel(r'C:\\Users\\GCV\\Documents\\MAS\\Project AR\\Testing\\data_files\\Consolidated_file_sorted.xlsx')\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "consolidate_df = pd.read_excel(r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/monetory_pbi_file_1.xlsx\")\n",
    "\n",
    "# Get unique customer names\n",
    "customer_names = consolidate_df[\"Customer\"].unique()\n",
    "#print(customer_names)\n",
    "\n",
    "dfs = []\n",
    "# Iterate over each customer and create a separate Excel file\n",
    "for customer in customer_names:\n",
    "    # Filter the dataframe for the current customer\n",
    "    #customer_df = consolidate_df[consolidate_df[\"Customer\"] == customer][[\"RFM_Score\", \"Customer_segment\", \"Week\", \"Cluster\",\"Monetory\"]]\n",
    "    customer_df1 = consolidate_df[consolidate_df[\"Customer\"] == customer][[\"Date\",\"Customer\", \"Week\",\"Monetory\",\"Week_num\",\"2022 Week\",\"2023 Week\",\"Monetary_2022\",\"Monetary_2023\",\"Total_Receivable_2022\",\"Total_Receivable_2023\",\"Future_prediction\",\"sMAPE\"]]\n",
    "    \n",
    "    \n",
    "    #add Week column which include W1 to W51 \n",
    "    weeks = ['W'+str(i) for i in range(1,52)]\n",
    "    oder =  [i for i in range(1,52)]\n",
    "    customer = customer_df1[\"Customer\"].iloc[1]\n",
    "    # create a dictionary with the column names and values\n",
    "    data = {'Customer': [str(customer)]*51, 'Week': weeks,'Oder':oder}\n",
    "\n",
    "    # create a dataframe from the dictionary\n",
    "    customer_df2 = pd.DataFrame(data)\n",
    "    # print(customer_df2.tail())\n",
    "    # print(customer_df1.tail())\n",
    "\n",
    "    full_join_df = pd.merge(customer_df2,customer_df1, on =\"Week\", how =\"outer\")\n",
    "    \n",
    "    #print(full_join_df.head())\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create output file name\n",
    "    output_file_name = f\"{customer}.xlsx\"\n",
    "    #output_folder = r\"C:/Users/GCV/Documents/MAS/Project AR/con_output/\"\n",
    "    output_folder1 = r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/test/\"\n",
    "   \n",
    "    #output_file_path = os.path.join(output_folder, output_file_name)\n",
    "    output_file_path1 = os.path.join(output_folder1, output_file_name)\n",
    "    full_join_df = full_join_df.drop('Customer_y', axis = 1)\n",
    "    dfs.append(full_join_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     #Assuming your DataFrame is named 'df' and the column is named 'b'\n",
    "#     index_last_non_null = full_join_df['Monetary_2023'].last_valid_index()\n",
    "#     #print(index_last_non_null)\n",
    "\n",
    "#     column_name_1 = 'Monetary_2023'\n",
    "#     column_name_2 = 'Future_prediction'\n",
    "\n",
    "\n",
    "#     value = full_join_df.loc[index_last_non_null, column_name_1]\n",
    "#     full_join_df.loc[index_last_non_null, column_name_2] = value\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    full_join_df.to_excel(output_file_path1, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1408f975-be55-4c25-832c-28569a999fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "input_folder = \"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/test/\"\n",
    "output_folder = \"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/test2/\"\n",
    "\n",
    "# Set the file pattern you want to match, for example, all files with \".xlsx\" extension\n",
    "file_pattern = input_folder + \"*.xlsx\"\n",
    "\n",
    "# Use glob function to get the file names that match each file pattern\n",
    "file_names = glob.glob(file_pattern)\n",
    "\n",
    "for filename in file_names:\n",
    "    try:\n",
    "        df = pd.read_excel(filename)  # Read in each file as a DataFrame\n",
    "\n",
    "        index_last_non_null = df['Monetary_2023'].last_valid_index()\n",
    "        #print(filename, index_last_non_null)\n",
    "\n",
    "        if index_last_non_null is None:\n",
    "            print(f\"Ignoring {filename} due to None values in the index.\")\n",
    "            continue  # Skip the current file and move to the next iteration\n",
    "\n",
    "        column_name_1 = 'Monetary_2023'\n",
    "        column_name_2 = 'Future_prediction'\n",
    "\n",
    "        value = df.loc[index_last_non_null, column_name_1]\n",
    "        df.loc[index_last_non_null, column_name_2] = value\n",
    "        value2 = df.loc[index_last_non_null, column_name_2]\n",
    "\n",
    "        # # Get the base name of the input file\n",
    "        base_name = os.path.basename(filename)\n",
    "        # Remove the ampersand character from the base name\n",
    "        #base_name = base_name.replace(\"&\", \"\")\n",
    "        # Create the output file path by joining the output folder and the modified base name\n",
    "        output_file_path = os.path.join(output_folder, base_name)\n",
    "        #print(output_file_path)\n",
    "        # Save the modified DataFrame to the output file\n",
    "        df.to_excel(output_file_path, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "        continue  # Skip the current file and move to the next iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d47c1856-b92e-417d-a2aa-d1d43c8f3c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "                          Customer Week  Oder        Date  Week_num 2022 Week  \\\n",
      "0  Abercrombie & Fitch Trading Co.   W1     1  2023-01-02       1.0       NaN   \n",
      "1  Abercrombie & Fitch Trading Co.   W2     2  2023-01-09       2.0       NaN   \n",
      "2  Abercrombie & Fitch Trading Co.   W3     3  2023-01-16       3.0       NaN   \n",
      "3  Abercrombie & Fitch Trading Co.   W4     4  2023-01-23       4.0       NaN   \n",
      "4  Abercrombie & Fitch Trading Co.   W5     5  2023-01-30       5.0       NaN   \n",
      "\n",
      "  2023 Week  Monetary_2022  Monetary_2023  Total_Receivable_2022  \\\n",
      "0        W1            NaN      349922.29                    NaN   \n",
      "1        W2            NaN      145975.80                    NaN   \n",
      "2        W3            NaN      147520.38                    NaN   \n",
      "3        W4            NaN      257317.71                    NaN   \n",
      "4        W5            NaN      268928.06                    NaN   \n",
      "\n",
      "   Total_Receivable_2023  Future_prediction  sMAPE  week_order  \n",
      "0              845774.58                NaN    NaN           1  \n",
      "1              664162.77                NaN    NaN           2  \n",
      "2              533017.78                NaN    NaN           3  \n",
      "3              572024.66                NaN    NaN           4  \n",
      "4              592716.93                NaN    NaN           5  \n"
     ]
    }
   ],
   "source": [
    "#getting final output excel file\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "input_folder =  \"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/test2/\"\n",
    "output_file = \"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/monetory_merged_predictions.xlsx\"\n",
    "\n",
    "# Set the file pattern you want to match, for example, all files with \".txt\" extension\n",
    "\n",
    "file_pattern= input_folder + \"*.xlsx\"\n",
    "\n",
    "# Use glob function to get the file names that match each file pattern\n",
    "file_names = glob.glob(file_pattern)\n",
    "\n",
    "\n",
    "# create empty dataframe to store merged data\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# loop through customer files and merge data\n",
    "for filename in file_names:   # replace file_list with the list of file names in your folder\n",
    "    df = pd.read_excel(filename)   # read in each file as a data frame\n",
    "\n",
    "    merged_df = pd.concat([merged_df, df], ignore_index=True)   # append each data frame to the merged data frame\n",
    "    \n",
    "\n",
    "merged_df = merged_df.drop('Monetory',axis =1)\n",
    "\n",
    "#merged_df = merged_df.fillna(0) #'Unnamed: 0.1','Unnamed: 0',\n",
    "#merged_df = merged_df.drop(['index','Customer'], axis=1)\n",
    "merged_df = merged_df.rename(columns={'Customer_x': 'Customer'})\n",
    "\n",
    "\n",
    "\n",
    "#make oder for each week to sort power bi dashboard\n",
    "week_order = {'W1': 1, 'W2': 2, 'W3': 3, 'W4': 4, 'W5': 5, 'W6': 6, 'W7': 7, 'W8': 8, 'W9': 9, 'W10': 10, 'W11': 11, 'W12': 12, 'W13': 13, 'W14': 14, 'W15': 15, 'W16': 16, 'W17': 17, 'W18': 18, 'W19': 19, 'W20': 20, 'W21': 21, 'W22': 22, 'W23': 23, 'W24': 24, 'W25': 25, 'W26': 26, 'W27': 27, 'W28': 28, 'W29': 29, 'W30': 30, 'W31': 31, 'W32': 32, 'W33': 33, 'W34': 34, 'W35': 35, 'W36': 36, 'W37': 37, 'W38': 38, 'W39': 39, 'W40': 40, 'W41': 41, 'W42': 42, 'W43': 43, 'W44': 44, 'W45': 45, 'W46': 46, 'W47': 47, 'W48': 48, 'W49': 49, 'W50': 50, 'W51': 51, 'W52': 52}\n",
    "\n",
    "merged_df['week_order'] = merged_df['Week'].map(week_order)\n",
    "#merged_df = merged_df.sort_values('week_order')\n",
    "\n",
    "#remove time part\n",
    "merged_df['Date'] = merged_df['Date'].dt.date\n",
    "# merged_df['Date_2022'] = merged_df['Date_2022'].dt.date\n",
    "# merged_df['Date_2023'] = merged_df['Date_2023'].dt.date\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#merged_df.to_excel(output_file)\n",
    "merged_df.to_excel(r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/monetory_pbi_file_final_1.xlsx\")\n",
    "print(\"done\")\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69ba29c6-a81c-49a6-a8e9-262e79e268d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Week', 'Customer', 'Predicted Values'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#adding validation line\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df1 = pd.read_excel(r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/monetory_pbi_file_final_1.xlsx\")\n",
    "df2 = pd.read_excel(r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/monetory_merged_predictions.xlsx\")\n",
    "\n",
    "df2 = df2[['Week', 'customer', 'Predicted Values']]\n",
    "df2 = df2.rename(columns={'customer': 'Customer'})\n",
    "df2 = df2.replace(0, np.nan)\n",
    "\n",
    "print(df2.columns)\n",
    "\n",
    "# merge df1 and df2 on 'Week' and 'Customer' columns\n",
    "result_df = pd.merge(df1, df2, on=['Week', 'Customer'], how='outer')\n",
    "\n",
    "#result_df['Date'] = result_df['Date'].dt.date\n",
    "#result_df = result_df.drop('Date', axis=1)\n",
    "if 'Unnamed: 0'in result_df.columns:\n",
    "    result_df = result_df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "result_df.to_excel(r\"C:/Users/GCV/Documents/MAS/Project AR/Testing/Demo/monetory_pbi_file_final.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab52849-afca-4d80-a53d-9ce9116a0801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
